{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8dcfd1d-fa55-4bff-b363-e908082515b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 向量模型下载\n",
    "from modelscope import snapshot_download\n",
    "model_dir = snapshot_download(\"AI-ModelScope/bge-small-zh-v1.5\", cache_dir='.')\n",
    "lora_model_dir = snapshot_download('IEITYuan/Yuan2-2B-Mars-hf', cache_dir='.')\n",
    "\n",
    "# 导入必要的库\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, TaskType, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9771f9fe-0900-462c-9476-817964b161ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 定义向量模型类\n",
    "class EmbeddingModel:\n",
    "    def __init__(self, path: str) -> None:\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "        self.model = AutoModel.from_pretrained(path).cuda()\n",
    "        print(f'Loading EmbeddingModel from {path}.')\n",
    "\n",
    "    def get_embeddings(self, texts: List[str]) -> List[float]:\n",
    "        encoded_input = self.tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
    "        encoded_input = {k: v.cuda() for k, v in encoded_input.items()}\n",
    "        with torch.no_grad():\n",
    "            model_output = self.model(**encoded_input)\n",
    "            sentence_embeddings = model_output[0][:, 0]\n",
    "        return torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1).tolist()\n",
    "\n",
    "### 向量库索引类，用于检索知识库\n",
    "class VectorStoreIndex:\n",
    "    def __init__(self, document_path: str, embed_model: EmbeddingModel) -> None:\n",
    "        self.documents = [line.strip() for line in open(document_path, 'r', encoding='utf-8')]\n",
    "        self.embed_model = embed_model\n",
    "        self.vectors = self.embed_model.get_embeddings(self.documents)\n",
    "        print(f'Loaded {len(self.documents)} documents from {document_path}.')\n",
    "\n",
    "    def get_similarity(self, vector1: List[float], vector2: List[float]) -> float:\n",
    "        return np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2) or 1)\n",
    "\n",
    "    def query(self, question: str, k: int = 1) -> List[str]:\n",
    "        question_vector = self.embed_model.get_embeddings([question])[0]\n",
    "        similarities = [self.get_similarity(question_vector, vector) for vector in self.vectors]\n",
    "        return [self.documents[i] for i in np.argsort(similarities)[-k:][::-1]]\n",
    "\n",
    "### 大语言模型类\n",
    "class LLM:\n",
    "    def __init__(self, model_path: str) -> None:\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path, add_eos_token=False, add_bos_token=False, eos_token='<eod>')\n",
    "        self.tokenizer.add_tokens(['<sep>', '<pad>', '<mask>'], special_tokens=True)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16, trust_remote_code=True).cuda()\n",
    "        print(f'Loaded LLM model from {model_path}.')\n",
    "\n",
    "    def generate(self, question: str, context: List[str] = None) -> str:\n",
    "        prompt = f'背景：{context}\\n问题：{question}\\n请基于背景，回答问题。' if context else question\n",
    "        inputs = self.tokenizer(prompt + \"<sep>\", return_tensors=\"pt\")[\"input_ids\"].cuda()\n",
    "        outputs = self.model.generate(inputs, do_sample=False, max_length=1024)\n",
    "        return self.tokenizer.decode(outputs[0]).split(\"<sep>\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8df70b8-1972-4490-b42a-9a906f6dc2f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "560f638edd6d4d43b975ec6a1a0fa732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/71 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### 加载数据集和LoRA配置\n",
    "df = pd.read_json('./metro_qa_data.json')\n",
    "ds = Dataset.from_pandas(df)\n",
    "tokenizer = AutoTokenizer.from_pretrained(lora_model_dir, add_eos_token=False, add_bos_token=False, eos_token='<eod>')\n",
    "tokenizer.add_tokens(['<sep>', '<pad>', '<mask>'], special_tokens=True)\n",
    "\n",
    "def process_func(example):\n",
    "    MAX_LENGTH = 384\n",
    "    question = tokenizer(f\"{example['question']}<sep>\")\n",
    "    answer = tokenizer(f\"{example['answer']}<eod>\")\n",
    "    input_ids = question[\"input_ids\"] + answer[\"input_ids\"]\n",
    "    attention_mask = [1] * len(input_ids)\n",
    "    labels = [-100] * len(question[\"input_ids\"]) + answer[\"input_ids\"]\n",
    "    return {\n",
    "        \"input_ids\": input_ids[:MAX_LENGTH],\n",
    "        \"attention_mask\": attention_mask[:MAX_LENGTH],\n",
    "        \"labels\": labels[:MAX_LENGTH]\n",
    "    }\n",
    "\n",
    "tokenized_id = ds.map(process_func, remove_columns=ds.column_names)\n",
    "model = AutoModelForCausalLM.from_pretrained(lora_model_dir, device_map=\"auto\", torch_dtype=torch.bfloat16, trust_remote_code=True)\n",
    "model.enable_input_require_grads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f9629f-e0b2-4e36-90c2-99d311dd557b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 4/18 00:00 < 00:01, 8.10 it/s, Epoch 0.50/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, \n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./output/Yuan2.0-2B_lora_bf16\",\n",
    "    per_device_train_batch_size=12,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=5e-5\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_id,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),\n",
    ")\n",
    "\n",
    "# 设置填充标记，若已添加 pad_token 可直接跳过\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "# 将填充标记设置为 [PAD] 或 eos_token\n",
    "tokenizer.pad_token = tokenizer.pad_token if tokenizer.pad_token else tokenizer.eos_token\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a8382b-13b4-4caf-9060-07ebeb7ab53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义 generate_lora 函数，使用 LoRA 模型生成回答\n",
    "def generate_lora(question, context=None):\n",
    "    # 将 context 转换为字符串\n",
    "    input_text = question if context is None else \" \".join(context) + \" \" + question\n",
    "    \n",
    "    # 使用 tokenizer 将输入文本转换为张量格式\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "    \n",
    "    # 使用 LoRA 模型生成回答，设置 max_new_tokens 以控制生成长度\n",
    "    output = model.generate(inputs, max_new_tokens=100)  # 设置适合的长度\n",
    "       \n",
    "    # 解码输出张量为可读文本\n",
    "    answer = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return answer\n",
    "\n",
    "\n",
    "\n",
    "def generate_combined_output(question):\n",
    "    print('> 1. 纯粹大模型：')\n",
    "    print(llm.generate(question))\n",
    "\n",
    "    print('> 2. 大模型 + RAG：')\n",
    "    context = index.query(question)\n",
    "    print(llm.generate(question, context))\n",
    "\n",
    "    print('> 3. 大模型 + LoRA：')\n",
    "    print(generate_lora(question))\n",
    "\n",
    "    print('> 4. 大模型 + RAG + LoRA：')\n",
    "    print(generate_lora(question, context))\n",
    "\n",
    "# 创建嵌入模型和索引\n",
    "embed_model = EmbeddingModel('./AI-ModelScope/bge-small-zh-v1___5')\n",
    "index = VectorStoreIndex('./metro_knowledge.txt', embed_model)\n",
    "\n",
    "# 创建大语言模型实例\n",
    "llm = LLM('./IEITYuan/Yuan2-2B-Mars-hf')\n",
    "\n",
    "# 测试生成\n",
    "test_question = \"地铁清分系统为什么失效？\"\n",
    "generate_combined_output(test_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6862645c-5c3a-4d4c-b59c-6ff31cc87643",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
