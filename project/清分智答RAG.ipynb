{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dae4a5e9-0bed-4979-b50f-a62f586b1062",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T08:10:42.770744Z",
     "iopub.status.busy": "2024-11-07T08:10:42.770406Z",
     "iopub.status.idle": "2024-11-07T08:10:55.780405Z",
     "shell.execute_reply": "2024-11-07T08:10:55.779897Z",
     "shell.execute_reply.started": "2024-11-07T08:10:42.770722Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a67c5ae39434b5a95847f13513add29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [1_Pooling/config.json]:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "838bc3ca42f740eb816ea67d2edecfca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [config.json]:   0%|          | 0.00/776 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06403a72f8cb42618a4422fc59e56d40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [config_sentence_transformers.json]:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dea11cc8b98a46a48b512448707419c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [configuration.json]:   0%|          | 0.00/47.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c592b6d329ac4cfc831984c048a47549",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [model.safetensors]:   0%|          | 0.00/91.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdc73554700a4c7c92fd52015e116d67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [modules.json]:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b2497b692f34c03b12893f75da7fed1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [pytorch_model.bin]:   0%|          | 0.00/91.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59f080f11e28431194e88c5615053aa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [README.md]:   0%|          | 0.00/27.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36c526e2bbe842b88953be51cce8aa02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [sentence_bert_config.json]:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2496b07780bc434e9cb5b5bc3ce75a1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [special_tokens_map.json]:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cbaa45ef4e44ff0aa42c00752e98a3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [tokenizer.json]:   0%|          | 0.00/429k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1c77ec5951649878749d7286c211276",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [tokenizer_config.json]:   0%|          | 0.00/367 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4386589360684692974f5470cbeb2296",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [vocab.txt]:   0%|          | 0.00/107k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 向量模型下载\n",
    "from modelscope import snapshot_download\n",
    "model_dir = snapshot_download(\"AI-ModelScope/bge-small-zh-v1.5\", cache_dir='.')\n",
    "\n",
    "# 导入必要库\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "173a40c3-1b2e-4524-93ed-8a1e7c174e6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T08:10:59.122854Z",
     "iopub.status.busy": "2024-11-07T08:10:59.122427Z",
     "iopub.status.idle": "2024-11-07T08:10:59.603153Z",
     "shell.execute_reply": "2024-11-07T08:10:59.602464Z",
     "shell.execute_reply.started": "2024-11-07T08:10:59.122831Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Create embedding model...\n",
      "Loading EmbeddingModel from ./AI-ModelScope/bge-small-zh-v1___5.\n"
     ]
    }
   ],
   "source": [
    "# 向量模型类，用于生成文本嵌入\n",
    "class EmbeddingModel:\n",
    "    def __init__(self, path: str) -> None:\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "        self.model = AutoModel.from_pretrained(path).cuda()\n",
    "        print(f'Loading EmbeddingModel from {path}.')\n",
    "\n",
    "    def get_embeddings(self, texts: List) -> List[float]:\n",
    "        encoded_input = self.tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
    "        encoded_input = {k: v.cuda() for k, v in encoded_input.items()}\n",
    "        with torch.no_grad():\n",
    "            model_output = self.model(**encoded_input)\n",
    "            sentence_embeddings = model_output[0][:, 0]\n",
    "        sentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)\n",
    "        return sentence_embeddings.tolist()\n",
    "\n",
    "# 创建向量模型实例\n",
    "print(\"> Create embedding model...\")\n",
    "embed_model_path = './AI-ModelScope/bge-small-zh-v1___5'\n",
    "embed_model = EmbeddingModel(embed_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27511515-146a-49dd-a44e-98ffef780f0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T08:11:17.184261Z",
     "iopub.status.busy": "2024-11-07T08:11:17.183896Z",
     "iopub.status.idle": "2024-11-07T08:11:18.015993Z",
     "shell.execute_reply": "2024-11-07T08:11:18.015436Z",
     "shell.execute_reply.started": "2024-11-07T08:11:17.184226Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Create index...\n",
      "Loaded 15 documents from ./metro_knowledge.txt.\n"
     ]
    }
   ],
   "source": [
    "# 向量库索引类，用于检索与问题相关的知识文本\n",
    "class VectorStoreIndex:\n",
    "    def __init__(self, document_path: str, embed_model: EmbeddingModel) -> None:\n",
    "        self.documents = []\n",
    "        for line in open(document_path, 'r', encoding='utf-8'):\n",
    "            line = line.strip()\n",
    "            self.documents.append(line)\n",
    "\n",
    "        self.embed_model = embed_model\n",
    "        self.vectors = self.embed_model.get_embeddings(self.documents)\n",
    "        print(f'Loaded {len(self.documents)} documents from {document_path}.')\n",
    "\n",
    "    def get_similarity(self, vector1: List[float], vector2: List[float]) -> float:\n",
    "        dot_product = np.dot(vector1, vector2)\n",
    "        magnitude = np.linalg.norm(vector1) * np.linalg.norm(vector2)\n",
    "        return dot_product / magnitude if magnitude else 0\n",
    "\n",
    "    def query(self, question: str, k: int = 1) -> List[str]:\n",
    "        question_vector = self.embed_model.get_embeddings([question])[0]\n",
    "        similarities = [self.get_similarity(question_vector, vector) for vector in self.vectors]\n",
    "        return [self.documents[i] for i in np.argsort(similarities)[-k:][::-1]]\n",
    "\n",
    "# 创建向量库索引\n",
    "print(\"> Create index...\")\n",
    "document_path = './metro_knowledge.txt'\n",
    "index = VectorStoreIndex(document_path, embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a70df80-d9dc-4862-ad98-e18e289d88c9",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-11-07T08:12:01.089991Z",
     "iopub.status.busy": "2024-11-07T08:12:01.089640Z",
     "iopub.status.idle": "2024-11-07T08:12:01.098997Z",
     "shell.execute_reply": "2024-11-07T08:12:01.098488Z",
     "shell.execute_reply.started": "2024-11-07T08:12:01.089969Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Question: 地铁清分系统为什么失效？\n",
      "> Context: ['清分系统可能失效的原因多种多样，包括但不限于账号信息不正确、账户密码失效、网络问题、系统升级失败、账户被盗以及电脑硬件故障等。这些问题可能导致清分系统无法正常工作，影响地铁的票务管理和客流分析。为了确保清分系统的有效运行，需要对这些潜在的问题进行识别和解决，同时采取有效的安全措施来保护系统免受网络攻击和数据泄露的威胁。']\n"
     ]
    }
   ],
   "source": [
    "# 测试问题\n",
    "question = '地铁清分系统为什么失效？'\n",
    "print('> Question:', question)\n",
    "\n",
    "# 从知识库中检索相关背景信息\n",
    "context = index.query(question)\n",
    "print('> Context:', context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4c5b364-a647-42c8-a122-385e60624121",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T08:12:15.743657Z",
     "iopub.status.busy": "2024-11-07T08:12:15.743313Z",
     "iopub.status.idle": "2024-11-07T08:12:30.557696Z",
     "shell.execute_reply": "2024-11-07T08:12:30.557133Z",
     "shell.execute_reply.started": "2024-11-07T08:12:15.743635Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Create Yuan2.0 LLM...\n",
      "Creating tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from ./IEITYuan/Yuan2-2B-Mars-hf.\n",
      "> Without RAG:\n",
      " 地铁清分系统失效可能是由于多种原因引起的。以下是一些可能的原因：\n",
      "1. 机械故障：清分系统中的机械部件可能由于磨损、摩擦或损坏而失效。例如，轨道上的传感器或执行器可能会出现故障，导致清分系统无法正常运作。\n",
      "2. 电子故障：清分系统中的电子部分可能由于电源故障、电路问题或电子元件损坏而失效。这可能导致系统无法正常启动、数据丢失或无法完成清分任务。\n",
      "3. 网络连接问题：清分系统通常通过地铁网络与控制中心进行通信。如果地铁网络出现故障，例如电力供应中断或网络连接中断，清分系统可能会受到干扰，导致无法正常完成任务。\n",
      "4. 安全系统触发：清分系统可能配置有安全触发机制，用于在检测到异常情况时自动停止或采取其他行动。如果安全系统检测到异常情况，清分系统可能被迫停止运行，直到安全事件得到解决。\n",
      "5. 人为错误：清分系统中的人工干预也可能导致失效。如果系统出现故障或错误，操作员可能未能正确配置或执行必要的操作，从而导致系统无法正常运行。\n",
      "要解决地铁清分系统的失效问题，需要首先确定具体的故障原因，然后采取相应的修复措施。这可能包括修理、更换损坏的部件、重新连接地铁网络或调整系统设置等。<eod>\n",
      "> With RAG:\n",
      " 地铁清分系统失效的原因可能包括但不限于账号信息不正确、账户密码失效、网络问题、系统升级失败、账户被盗以及电脑硬件故障等。这些问题可能导致清分系统无法正常工作，影响地铁的票务管理和客流分析。为了确保清分系统的有效运行，需要对这些潜在的问题进行识别和解决，同时采取有效的安全措施来保护系统免受网络攻击和数据泄露的威胁。<eod>\n"
     ]
    }
   ],
   "source": [
    "# 大语言模型类，用于生成基于上下文的回答\n",
    "class LLM:\n",
    "    def __init__(self, model_path: str) -> None:\n",
    "        print(\"Creating tokenizer...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path, add_eos_token=False, add_bos_token=False, eos_token='<eod>')\n",
    "        self.tokenizer.add_tokens(['<sep>', '<pad>', '<mask>', '<predict>', '<FIM_SUFFIX>', '<FIM_PREFIX>', '<FIM_MIDDLE>', '<commit_before>', '<commit_msg>', '<commit_after>', '<jupyter_start>', '<jupyter_text>', '<jupyter_code>', '<jupyter_output>', '<empty_output>'], special_tokens=True)\n",
    "\n",
    "        print(\"Creating model...\")\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16, trust_remote_code=True).cuda()\n",
    "        print(f'Loaded model from {model_path}.')\n",
    "\n",
    "    def generate(self, question: str, context: List):\n",
    "        if context:\n",
    "            prompt = f'背景：{context}\\n问题：{question}\\n请基于背景，回答问题。'\n",
    "        else:\n",
    "            prompt = question\n",
    "        prompt += \"<sep>\"\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].cuda()\n",
    "        outputs = self.model.generate(inputs, do_sample=False, max_length=1024)\n",
    "        output = self.tokenizer.decode(outputs[0])\n",
    "        print(output.split(\"<sep>\")[-1])\n",
    "\n",
    "# 创建大语言模型实例\n",
    "print(\"> Create Yuan2.0 LLM...\")\n",
    "model_path = './IEITYuan/Yuan2-2B-Mars-hf'\n",
    "llm = LLM(model_path)\n",
    "\n",
    "# 测试生成\n",
    "print('> Without RAG:')\n",
    "llm.generate(question, [])\n",
    "\n",
    "print('> With RAG:')\n",
    "llm.generate(question, context)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
